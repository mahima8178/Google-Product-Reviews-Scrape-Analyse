{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsSpftIVFChSzU/rYBBpdj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahima8178/Google-Product-Reviews-Scrape-Analyse/blob/main/Google_Product_Reviews_Scrape%26Analyse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "5FN8gBD77I-d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqKRt__F248e",
        "outputId": "3f2d07b4-7e75-4881-bc32-647c37755af6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stqdm in /usr/local/lib/python3.10/dist-packages (0.0.5)\n",
            "Requirement already satisfied: streamlit>=0.66 in /usr/local/lib/python3.10/dist-packages (from stqdm) (1.27.1)\n",
            "Requirement already satisfied: tqdm>=4.50 in /usr/local/lib/python3.10/dist-packages (from stqdm) (4.66.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit>=0.66->stqdm) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (5.3.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (6.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (23.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (13.5.2)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (5.0.1)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (0.22.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (3.1.37)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (0.8.1b0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (6.3.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.66->stqdm) (3.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.66->stqdm) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.66->stqdm) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.66->stqdm) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.66->stqdm) (0.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.66->stqdm) (4.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit>=0.66->stqdm) (3.16.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.66->stqdm) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit>=0.66->stqdm) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.66->stqdm) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.66->stqdm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.66->stqdm) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.66->stqdm) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.66->stqdm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.66->stqdm) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.66->stqdm) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=0.66->stqdm) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.66->stqdm) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.66->stqdm) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.66->stqdm) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.66->stqdm) (0.10.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=0.66->stqdm) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apify-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm5wSi07FzKu",
        "outputId": "9f992105-1321-4110-d2b9-e507aaf0e10d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apify-client in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: apify-shared~=1.0.0 in /usr/local/lib/python3.10/dist-packages (from apify-client) (1.0.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from apify-client) (0.25.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->apify-client) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->apify-client) (0.18.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->apify-client) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->apify-client) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx>=0.24.1->apify-client) (3.7.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx>=0.24.1->apify-client) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.19.0,>=0.18.0->httpx>=0.24.1->apify-client) (1.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
      ],
      "metadata": {
        "id": "v3Fu1hHy_Za6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the models with the tokenizer\n",
        "tokenizer_sent = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model_sent = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
      ],
      "metadata": {
        "id": "11pgUCo99eiQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "6-tLQcyFACUL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "_69Aozi_ALNP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "import streamlit as st\n",
        "import nltk\n",
        "\n",
        "MODEL_NAME_AMAZON = \"LiYuan/amazon-review-sentiment-analysis\"\n",
        "MODEL_NAME_TWITTER = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "AMAZON_ROW_LABELS = ['1 star', '2 stars', '3 stars', '4 stars', '5 stars']\n",
        "TWITTER_ROW_LABELS = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "AMAZON_ROW_VALUES = list(range(1,6))\n",
        "TWITTER_ROW_VALUES = [0, 0.5, 1]\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def download_model(model_name: str = MODEL_NAME_AMAZON):\n",
        "    '''\n",
        "    Downloads the model from huggingface.\n",
        "    '''\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    device = torch.cuda.current_device() if torch.cuda.is_available() else None\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
        "    return nlp\n",
        "\n",
        "@st.cache_resource\n",
        "def init():\n",
        "    '''\n",
        "    Downloads the stopwords for nltk.\n",
        "    '''\n",
        "    nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejIxQSp3H5Nm",
        "outputId": "f6999fe0-9ed4-4ad8-ef26-72f361ef5e75"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-8a0r4CH7vv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scraper.py\n",
        "import requests, json, datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import streamlit as st\n",
        "from apify_client import ApifyClient\n",
        "APIFY_TOKEN='apify_api_esQN883jVZCZCBhLP0CvrcQidXHeAh3ukmtI'\n",
        "\n",
        "def gt(dt_str):\n",
        "     '''\n",
        "     Converts an isoformat string to a datetime object.\n",
        "     '''\n",
        "     dt, _, us = dt_str.partition(\".\")\n",
        "     dt = datetime.datetime.strptime(dt, \"%Y-%m-%dT%H:%M:%S\")\n",
        "     us = int(us.rstrip(\"Z\"), 10)\n",
        "     return dt + datetime.timedelta(microseconds=us)\n",
        "\n",
        "def query_for_usage():\n",
        "    '''\n",
        "    Queries Apify for the number of queries used.\n",
        "    '''\n",
        "    print('Querying for usage...')\n",
        "    url = 'https://api.apify.com/v2/users/me/usage/monthly?token=' + APIFY_TOKEN\n",
        "    r = requests.get(url)\n",
        "    d = json.loads(r.text)\n",
        "    date = gt(d['data']['usageCycle']['endAt'])\n",
        "    date_diff = (date - datetime.datetime.now())\n",
        "    return f\"**{d['data']['monthlyServiceUsage']['PROXY_SERPS']['quantity']*100} / 50,000** queries used this month. Resets in **{date_diff.days} days, {date_diff.seconds//3600} hours**.\"\n",
        "\n",
        "@st.cache_data\n",
        "def query_google(query: str, num_of_queries: int, use_json=True):\n",
        "    '''\n",
        "    Queries Google for a given query.\n",
        "    Returns a list of descriptions and a list of ratings.\n",
        "    '''\n",
        "    print(f'Searching for {num_of_queries} reviews...')\n",
        "    if use_json:\n",
        "        with open('data.json') as f:\n",
        "            res_ls = json.load(f)\n",
        "    else:\n",
        "        client = ApifyClient(APIFY_TOKEN)\n",
        "\n",
        "        run_input = { \"queries\": f\"{query} review\",\n",
        "                    \"maxPagesPerQuery\": num_of_queries // 100,\n",
        "                    \"resultsPerPage\": 100,\n",
        "                    \"countryCode\": \"\",\n",
        "                    \"customDataFunction\": \"\"\"async ({ input, $, request, response, html }) => {\n",
        "                    return {\n",
        "                    pageTitle: $('title').text(),\n",
        "                    };\n",
        "                };\"\"\",\n",
        "                }\n",
        "        run = client.actor(\"apify/google-search-scraper\").call(run_input=run_input)\n",
        "        res_ls = []\n",
        "        for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
        "            res_ls.append(item)\n",
        "\n",
        "\n",
        "    rating_dataset = []\n",
        "    desc_dataset = []\n",
        "    for res in res_ls:\n",
        "        # res is a dictionary\n",
        "        for row in res['organicResults']:\n",
        "            try:\n",
        "                rating = float(row['productInfo']['rating'])\n",
        "                if rating > 10:\n",
        "                    rating = rating/100\n",
        "                elif rating > 5:\n",
        "                    rating = rating/10\n",
        "                else:\n",
        "                    rating = rating/5\n",
        "                rating_dataset.append(rating)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            desc = row['description'].replace('\\xa0','')\n",
        "            if len(desc) > 14:\n",
        "\n",
        "                # Check case for Nov XX, XXXX\n",
        "                if desc[12].isalpha() and desc[8:12].isdigit():\n",
        "                    desc = desc[12:]\n",
        "\n",
        "                # Check case for Nov X, XXXX\n",
        "                elif desc[11].isalpha() and desc[7:11].isdigit():\n",
        "                    desc = desc[11:]\n",
        "\n",
        "                desc_dataset.append(desc)\n",
        "\n",
        "\n",
        "    return desc_dataset, np.array(rating_dataset, dtype=np.float32)\n",
        "\n",
        "\n",
        "def create_wordcloud(desc_dataset: list):\n",
        "    '''\n",
        "    Creates a wordcloud from a list of descriptions.\n",
        "    '''\n",
        "    pos_words = ''\n",
        "    neg_words = ''\n",
        "    neutral_words = ''\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for desc in desc_dataset:\n",
        "        for word in desc.split():\n",
        "            word = word.lstrip(punctuation).rstrip(punctuation)\n",
        "            if word:\n",
        "                analysis = TextBlob(word)\n",
        "                if analysis.sentiment.polarity > 0:\n",
        "                    pos_words += word + ' '\n",
        "                elif analysis.sentiment.polarity < 0:\n",
        "                    neg_words += word + ' '\n",
        "                else:\n",
        "                    neutral_words += word + ' '\n",
        "\n",
        "    figs = []\n",
        "    for title, words in [('Positive words', pos_words), ('Negative words', neg_words), ('Neutral words', neutral_words)]:\n",
        "        wordcloud = WordCloud(background_color ='white',\n",
        "                        stopwords = stop_words,\n",
        "                        min_font_size = 10,\n",
        "                        max_words = 20).generate(words)\n",
        "\n",
        "        # Plot the WordCloud image\n",
        "        fig, ax = plt.subplots(figsize=(15,5))\n",
        "        ax.set_facecolor('black')\n",
        "        plt.imshow(wordcloud)\n",
        "        plt.title(title, fontsize=55, color='black', pad=40)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "        figs.append(fig)\n",
        "    return figs\n",
        "\n",
        "\n",
        "\n",
        "def show_ratings(rating_dataset, rating_round=10, plot_type='line'):\n",
        "    '''\n",
        "    Shows a chart of the pure ratings.\n",
        "    rating_round: 0 = Continuous (0-1), 5 = Integer /5, 10 = Integer /10\n",
        "    plot_type: 'Line', 'Scatter', 'Both' <- For continuous only\n",
        "    '''\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    if rating_round == 5 or rating_round == 10:\n",
        "        if rating_round == 5:\n",
        "            rating_dataset = np.vectorize(lambda x: round(x * 5))(rating_dataset)\n",
        "        else:\n",
        "            rating_dataset = np.vectorize(lambda x: round(x * 10))(rating_dataset)\n",
        "\n",
        "        # Count occurrences of each loan\n",
        "        df = pd.Series(rating_dataset)\n",
        "        df2 = df.value_counts()\n",
        "        df2 = df2.reindex(list(range(1, rating_round + 1)))\n",
        "        df2.plot(ax=ax, kind='bar')\n",
        "\n",
        "    else:\n",
        "        x ,y  = np.unique(rating_dataset, return_counts=True)\n",
        "        if plot_type == 'Line':\n",
        "            plt.plot(x,y)\n",
        "        elif plot_type == 'Scatter':\n",
        "            plt.scatter(x,y)\n",
        "        elif plot_type == 'Both':\n",
        "            plt.plot(x,y)\n",
        "            plt.scatter(x,y)\n",
        "        plt.xlim(xmin=0, xmax=1)\n",
        "    plt.title('Raw Ratings', fontsize=15, color='black')\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def eval_sentiment(desc_dataset, model, row_labels, row_values, title):\n",
        "    '''\n",
        "    Shows a chart of the sentiment of the descriptions.\n",
        "    '''\n",
        "    res = model(desc_dataset)\n",
        "    df = pd.DataFrame(res)['label'].str.capitalize()\n",
        "    fig, ax = plt.subplots(figsize=(8,8))\n",
        "    df2 = df.value_counts()\n",
        "    df2 = df2.reindex(row_labels)\n",
        "    df2.plot(ax=ax, kind='bar')\n",
        "    plt.title(title, fontsize=25, color='black')\n",
        "    plt.xticks(fontsize=25)\n",
        "    plt.yticks(fontsize=25)\n",
        "    plt.show()\n",
        "\n",
        "    total = 0\n",
        "    count = 0\n",
        "    total = 0\n",
        "    for df2_val, row_val in zip(df2, row_values):\n",
        "        if not pd.isna(df2_val):\n",
        "            total += row_val * df2_val\n",
        "            count += df2_val\n",
        "\n",
        "    mean = total / len(desc_dataset)\n",
        "\n",
        "\n",
        "    return fig, mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8okilGEiFrFR",
        "outputId": "cd54df03-fd7f-461b-ed04-25ecfab20f03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scraper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "import scraper\n",
        "import utils\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "### INITIALIZATION ###\n",
        "# if \"load_state\" not in st.session_state:\n",
        "#      st.session_state.load_state = False\n",
        "if \"rating_round\" not in st.session_state:\n",
        "     st.session_state.rating_round = 'Continuous (0 to 1)'\n",
        "if \"plot_type\" not in st.session_state:\n",
        "     st.session_state.plot_type = 'Line'\n",
        "\n",
        "twitter_model = utils.download_model(utils.MODEL_NAME_TWITTER)\n",
        "amazon_model = utils.download_model(utils.MODEL_NAME_AMAZON)\n",
        "\n",
        "utils.init()\n",
        "\n",
        "\n",
        "### MAIN APP ###\n",
        "st.title('Review Scraper')\n",
        "st.caption('Scrapes [Google](https://www.google.com/) for reviews of a product')\n",
        "\n",
        "st.caption('Powered by [HuggingFace](https://huggingface.co/), [Streamlit](https://streamlit.io/), and [Apify](https://apify.com/)')\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "div[class*=\"stRadio\"] > label > div[data-testid=\"stMarkdownContainer\"] > p {\n",
        "    font-size: 17px;\n",
        "\n",
        "}\n",
        ".big-font {\n",
        "    font-size: 18px;\n",
        "}\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "st.warning('Apify has a limit of 50,000 queries a month', icon=\"⚠️\")\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    query_usage = st.button('See current usage')\n",
        "if query_usage:\n",
        "    with col2:\n",
        "        st.write(scraper.query_for_usage())\n",
        "\n",
        "\n",
        "form = st.form(key='main_form')\n",
        "item = form.text_input('Enter an item (product, movie, etc.) to search for reviews: ', value='PS5 Console')\n",
        "num_of_queries = form.slider('Number of queries: ', min_value=100, max_value=1000, value=100, step=100)\n",
        "submit = form.form_submit_button('Search')\n",
        "if submit:\n",
        "    st.write(f'Searching for reviews for **{item}**...')\n",
        "    desc_dataset, rating_dataset = scraper.query_google(item, num_of_queries, use_json=False) # use_json=True for dummy data\n",
        "    st.write(f'<span class=big-font>Found `{len(desc_dataset)}` reviews</span>', unsafe_allow_html = True)\n",
        "    st.write(f'<span class=big-font>Found `{len(rating_dataset)}` ratings</span>', unsafe_allow_html = True)\n",
        "\n",
        "    if len(rating_dataset) > 0:\n",
        "        st.markdown(\"***\")\n",
        "        st.subheader('Raw ratings')\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            rating_round_d = {'Continuous (0 to 1)': 0, 'Out of 5': 5, 'Out of 10': 10}\n",
        "            st.session_state.rating_round = st.radio('Rating precision:', rating_round_d.keys(), index=0)\n",
        "\n",
        "        with col2:\n",
        "            plot_types_ls = ['Line', 'Scatter', 'Both']\n",
        "            st.session_state.plot_type = st.radio('Chart type (for continuous only):', plot_types_ls, index=2)\n",
        "\n",
        "        fig = scraper.show_ratings(rating_dataset, rating_round=rating_round_d[st.session_state.rating_round], plot_type=st.session_state.plot_type)\n",
        "        st.pyplot(fig)\n",
        "        multiplier = 1 if st.session_state.rating_round == 'Continuous (0 to 1)' else rating_round_d[st.session_state.rating_round]\n",
        "        st.write(f'<span class=big-font><b>Average <u>raw</u> rating:</b> `{np.mean(rating_dataset) * multiplier: .2f} / {multiplier}`</span>', unsafe_allow_html = True)\n",
        "\n",
        "    if len(desc_dataset) > 0:\n",
        "        st.markdown(\"***\")\n",
        "        st.subheader('Sentiment Analysis')\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            fig, mean = scraper.eval_sentiment(desc_dataset, amazon_model, utils.AMAZON_ROW_LABELS, utils.AMAZON_ROW_VALUES, 'Rating from 1 - 5 stars')\n",
        "            st.pyplot(fig)\n",
        "            st.write(f'<span class=big-font><b>Average rating from <u>1 - 5</u>:</b> `{mean: .2f} / 5`</span>', unsafe_allow_html = True)\n",
        "        with col2:\n",
        "            fig, mean = scraper.eval_sentiment(desc_dataset, twitter_model, utils.TWITTER_ROW_LABELS, utils.TWITTER_ROW_VALUES, 'Negative / Neutral / Positive')\n",
        "            st.pyplot(fig)\n",
        "            st.write(f'<span class=big-font><b>Average rating from <u>0 / 0.5 / 1</u>:</b> `{mean: .2f} / 1`</span>', unsafe_allow_html = True)\n",
        "\n",
        "        st.markdown(\"***\")\n",
        "        wc_figures = scraper.create_wordcloud(desc_dataset)\n",
        "        st.subheader('Word clouds')\n",
        "        col3, col4, col5 = st.columns(3)\n",
        "        with col3:\n",
        "            st.pyplot(wc_figures[0])\n",
        "        with col4:\n",
        "            st.pyplot(wc_figures[1])\n",
        "        with col5:\n",
        "            st.pyplot(wc_figures[2])\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbn45wkRFrOk",
        "outputId": "46cd4848-186d-4adb-a20b-5103aeecbb99"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run main.py &>/content/logs.txt &\n",
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "!npx localtunnel --port 8501 wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SymlnWi-9esa",
        "outputId": "de874b99-e4c7-43e7-aaf6-4a68bedf83a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.125.184.58\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.08s\n",
            "your url is: https://curly-pandas-open.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wuNyDZCg9exy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bf_0A5-57kkX"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}